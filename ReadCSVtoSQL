##Load data into Azure SQL Database from Azure Databricks using Python#

jdbcHostname = "azsqlserver.database.windows.net"
jdbcPort = "1433"
jdbcDatabase = "azsqldb"
properties = {
 "user" : "santosh",
 "password" : "******" }

 url = "jdbc:sqlserver://{0}:{1};database={2}".format(jdbcHostname,jdbcPort,jdbcDatabase)
mydf = sqlContext.read.csv("/FileStore/tables/1000_Sales_Records-d540d.csv",header=True)

from pyspark.sql import *
import pandas as pd
myfinaldf = DataFrameWriter(mydf)
myfinaldf.jdbc(url=url, table= "TotalProfit", mode ="overwrite", properties = properties)
--------------------------------

import os
from datetime import datetime
path = '/dbfs/'
fdpaths = [path+"/"+fd for fd in os.listdir(path)]
print(" file_path " + " create_date " + " modified_date ")
for fdpath in fdpaths:
  statinfo = os.stat(fdpath)
  create_date = datetime.fromtimestamp(statinfo.st_ctime)
  modified_date = datetime.fromtimestamp(statinfo.st_mtime)
  print(fdpath, create_date, modified_date)
---------------------------------------------------
dbutils.notebook.run(".\MountADL",60)
------------------------------------------------


create secret scope in azure and call them from adb notebook secrets
%scala
val jdbcUsername = dbutils.secrets.get(scope = "SQLCredentials", key = "AZSQLDBUsername")
val jdbcPassword = dbutils.secrets.get(scope = "SQLCredentials", key = "AZSQLDBPassword")

class.forName("com.microsoft.sqlserver.jdbc.SQLServerDriver")

val jdbcHostname = "azsqlserver21.database.windows.net"
val jdbcPort = 1433
val jdbcDatabase = "azdb"

val jdbcUrl = s"jdbc:sqlserver://${jdbcHostname}:${jdbcPort};database=${jdbcDatabase}

import java.util.Properties
val connectionProperties = new Properties()


connectionProperties.put("user",s"${jdbcUsername}")
connectionProperties.put("password",s"${jdbcPassword}")

val driverclass = "com.com.microsoft.sqlserver.jdbc.SQLServerDriver"
connectionProperties.setProperty("Driver",driverclass)


val azsqlproduct = spark.read.jdbc(jdbcUrl,"SalesLT.Product",connectionProperties)
azsqlproduct.show()


df = spark.read.format("csv").option("header","true").load("dbfs:/mnt/files/file2.txt")
df.write
  .format("jdbc")
  .mode("overwrite")
  .option("driver","com.microsoft.sqlserver.jdbc.SQLServerDriver"),
  .option("url",jdbcUrl)
  .option("dbtable","tbl1")
  .option("user",jdbcUsername)
  .option("password",jdbcPassword)
  .save()
